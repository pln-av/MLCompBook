# Basic Linear Algebra Operations and Definitions
[comment]: <> (Definition from https://golem.ph.utexas.edu/wiki/instiki/show/Theorems)
Here I want to introduce some notation that I use (which may be non-standard) and some ideas 
which are basic and well known, but helpful to keep in mind when reading the ML literature.
The vector $\mb{u}$ with $n$ elements is either a horizontal or vertical collection of $n$ 
numbers.  In the former $\mb{u}$ is *row* vector (and I note its dimension 
$\mb{u}\equiv\msize{1}{n}$) and the latter it is a *column* vector ($\mb{u}\equiv\msize{n}{1}$).
By default all vectors are column vectors.  A column vector $\mb{u}$ can be written as a row
by tranposing (ie $\transpose{\mb{u}}$).  We define the two vector-vector products:

###### *Vector Products*
The *dot product* of two vectors $\mb{u}$ and $\mb{v}$ is the scalar
\eq{
    \mb{u}\cdot\mb{v} = \transpose{\mb{u}}\mb{v} = \sum \mb{u}_i \mb{v}_i
} 
The *vector product* of two vectors $\mb{u}$ and $\mb{v}$ is the matrix
\eq{
    \mb{u}\transpose{\mb{v}} = 
    \mat{
         u_1  v_1 & u_1  v_2 & \hdots & u_1  v_n \\
         u_2  v_1 & u_2  v_2 & \hdots & u_2  v_n \\
         \vdots & \vdots & \ddots & \vdots \\
         u_n  v_n & u_1  v_n & s & u_n  v_n
    } 
}   

The vectors $\mb{u}$
A $m\times n$ matrix $A$, where 
\eq{A = \matx{A}{m}{n}}
is often decomposed into its $m$ rows 
\eq{
    A = \colx{ \row{A} }{m}
}
or $n$ columns
\eq{
    A = \rowx{ \col{A} }{n}
}
so as to better represent matrix operations.  With this notation we view the matrix product $AB$ where 
$A\equiv\left(m\times n\right)$ and $B\equiv\left(n\times m\right)$ as a matrix of dot products
\eq{
    AB = 
    \mat{
      \row{A}_1 \cdot \col{B}_1 & \row{A}_1 \cdot \col{B}_2 & \cdots & \row{A}_1 \cdot \col{B}_m \\
     \row{A}_2 \cdot \col{B}_1 & \row{A}_2 \cdot \col{B}_2 & \cdots & \row{A}_2 \cdot \col{B}_m \\
     \vdots & \vdots & \ddots & \vdots \\
     \row{A}_m \cdot \col{B}_m & \row{A}_1 \cdot \col{B}_m & \cdots & \row{A}_m \cdot \col{B}_m
    } 
}

The matrix-vector product $A\mb{x}$ is the weighted sum of the columns of $A$ ie
\eq{
    A\mb{x} = \sum x_i \col{A}_i
}
The set of all possible combinations of the columns defines the *column space* of A.  This idea
is couple to the idea of a *vector space*.  More specifically, the *vector space* generated by a 
set of $n$ vectors $\left\{\mb{u}_1,\hdots,\mb{u}_n\right\}$ is the set of all possible linear
combinations of the $\mb{u}_i$.  If we combine these vectors to form the columns of a matrix $A$,
the we get the linear combination from the above matrix-vector product $A\mb{x}$.  The column 
space of a matrix $A$ then is the *vector space* generated by its columns.  This *vector space*,
which is the *column space* of $A$ is often also called the *range* of $A$, ie
\eq{
    \textrm{range}\left(A\right) 
    = \left\{ \mb{u}\in\mc{R}^n \; \textrm{where there exists} \; \mb{x} \; \textrm{with} \; A\mb{x}=\mb{u} \right\}
}
since it defines which vectors can be produced by summing the columns in $A$.  The *null space*
is a particular set of non-zero weights $\mb{x}$ which produce the zero vector, ie
\eq{
    \textrm{null}\left(A\right) 
    = \left\{ \textrm{all} \; \mb{u} \; \textrm{where} \; A\mb{u}=0 \right\}
}
If such an $\mb{u}$ exists, then one column can be written as a linear combination of at 
least one of the others.  This has significant implications - see below.  If no such combination
exists, then the columns are *linearly independant* - one cannot be written as a combination
of the others.  This concept is again common to sets of vectors - ie a set of vectors is
linearly independant if no linear combination of the vectors produces a zero vector.  Spaces
which are *not* linearly independant include redundant vectors.  

The *column rank* of matrix $A$ is the largest number of independant columns in $A$.  If all
columns are independant, the matrix has *full rank*.  It is denoted $\textrm{colrank}\left(A\right)$.
The *row rank* is defined similarly, but is used much less frequently - the *rank* of a matrix
$\textrm{rank}\func{A}$ usually refers to the column rank.  Some examples would be good.

Consider a matrix $A$ and some set of vectors $\rowx{\mb{u}}{n}$.  For every possible combination
of the columns of $A$ (ie every $\mb{v}=A\mb{u}$), it may be possible to choose some combination
of the vectors $\mb{u}_i$ that produces the same $\mb{v}$, and vica versa.  In this sense, the set of 
vectors$\rowx{\mb{u}}{n}$ can substitute for the matrix $A$, since they can both generate the exact same
output (ie they have the same range).  If we want to do this substitution, we shouldnt choose any 
redundant vectors in the $\mb{u}_i$ - we should only use *linearly independant* vectors.  Such
a set of vectors is said to form a *basis* for $A$.

## Elementary Matrices and Elementary Matrix operations

Here a section on elementary matrices, and elementary operations.  

Consider two types of elementary matrices:

* Diagonal or Scaling matrices
* Permutation matrices

Consider a matrix $A$.  Multiplication by an elementary matrix $E$ transforms either rows or columns of 
its operand depending on whether we pre or post multiply ie $EA$ or $AE$.
 
 Operation | Effect
 ----------|---------
 pre-multiply (ie $EA$) | row transformation
 post-multiply (ie $AE$) | column transformation 

## Non-Singular Matrices

Consider the inverse problem.  Given a set of vectors $\rowx{\mb{u}}{n}$, or a matrix $A$, is it 
possible to compute out which combination produces a particular vector $\mb{v}$?   That is, can we 
solve $A\mb{x}=\mb{v}$?  Clearly, $\mb{v}$ should be in column space of $A$, since the column space
contains all possible linear combinations of the columns in $A$.  Under what conditions can we 
find this $\mb{x}$?

The square matrix $A$ is *non-singular* if a unique solution $\mb{x}$ to $A\mb{x}=\mb{v}$ exists.
The solution is denoted $\mb{x} = \inverse{A}\mb{v}$.  If $\inverse{A}$ exists, then it is unique.  To see this,
assume instead that $\inverse{A}=B_1$ and $\inverse{A}=B_2$ where $B_1\neq B_2$ - that is, there are
two different matrices which are inverses of $A$.  By definition
\eqa{
    AB_1 & = I = B_1 A \\
    AB_2 & = I = B_2 A 
}
and by multiplying the second equation by $B_1$ we see 
\eq{
    \left(B_1 A\right)B_2 = B_1 I 
}
where the bracketed term is $I$ and we have $B_2=B_1$.  The previous definitions of *rank*, *range*
and *null space* are usefull here.  When determing if $\inverse{A}$ exists, remember
\eqa{
    \inverse{A} & \iff \textrm{null}\func{A} = \left\{ 0 \right\} \\
    \inverse{A} & \iff \textrm{range}\func{A} = \mc{R}^n \\
    \inverse{A} & \iff \textrm{rank}\func{A} = n
 }
 Some important identities that are used regularly are
 \eqa{
     \inverse{\func{AB}} & = \transpose{B}\transpose{A} \\
     \inverse{\func{\transpose{A}}} & = \transpose{\func{\inverse{A}}}
 }
 To check the first, let $M=AB$, and note that 
 \eqa{
     \inverse{B}\inverse{A}M & = \inverse{B}\func{\inverse{A}A}B  = I\\
 }
 so that $\inverse{M} = \inverse{B}\inverse{A} = \inverse{\func{AB}}$. For the 
 second, consider $\transpose{\func{\inverse{A}}}\transpose{A} = \transpose{\func{A \inverse{A}}} = I$.

Some examples would be good here, as well as elementary matrix operations section.  Also, a section
on why we do not computing inverse matrices directly.  Conditioning etc.

# Some Important Matrices

In these sections, I need to flesh out the proofs and provide some examples.  How do I nicely input
definitions and proofs into this?

### Diagonal Matrices

 Diagonal matrices $D$ are matrices where all off-diagonal elements are zero ie $D_{ij}\neq0$ only
 for $i=j$.  In these notes assume unless otherwise specified that $D$ is diagonal.  If $D$ is 
 diagonal, then $\transpose{D}=D$ since it is symetric.  Also, both
 $D_1 and D_2$ and $D_1 D_2$ are diagonal.  To see that the product is diagonal, Let $A$ and 
 $B$ be diagonal.  Then $\func{AB}_{ij} = \row{A}_i \col{B}_j$.  Only the $i^{\textrm{th}}$ element in
 $\row{A}_i \neq 0$, and the $j^{\textrm{th}}$ element in $\col{B}_j \neq 0$.  Therefore a dot
 product can only be non-zero when $i=j$.

 *Diagonal matrices are a scaling operation*.  As with other *elementary matrices*, pre-multiplying
 is a row transformation, post-multiplying is a column transformation. To see this, recognise the
 matrix product as a matrix of dot products.  In the case of pre-multiplication, where $D$ is 
 diagonal with  $D_{ii} = d{i}$
 \eqa{
     \func{DA}_{ij} & = \row{D}_i \cdot \col{A}_j \\
                    & = D_{ii} A_{ij} \\
                    & = d_i A_{ij}
 } 
where the second line occurs because all elements in $\row{D}_i=0$ except the $\ith{i}$ element, 
so only the $\ith{i}$ element in $\col{A}_i$ (ie $A_{ij}$), survives the dot product.  The result is the 
whole of the $\ith{i}$ row of $A$ is multiplied through by $d_i$.  The analgous result happens 
for post-multiplication.  This has important implications - if $D$ is diagonal and 
$D_{ii}\neq  0 \; \forall \; i$ then $\inverse{D}$ exists, and each element of $\inverse{D}$ is $1/D_{ii}$.  
Note too the the determinant of a diagonal matrix is
\eq{
    \determinant{D} = \prod D_{ii}
}
which is clearly zero if any diagonal element is zero.

### Triangular Matrices

Triangular matrices are ubiquitious in computation linear algebra, because they can be solved with
little effort.  Matrices can be upper or lower triangular:

* *Upper Triangular:* $U$ is upper triangular if all $U_{ij}=0$ when $i<j$
* *Lower Triangular:* $L$ is lower triangular if all $L_{ij}=0$  when $i>j$

I often refer to `lower` and `upper` as being `types` of triangular matrices.  Assume everywhere that $T$ 
is a triangular matrix of some type, and $U$/$L$ are upper/lower triangular matrices unless otherwise 
specified. There are some interesting properties of triangular matrices.  

* If $T_1,T_2$ have the same type, then the sum $T = T_1 + T_2$ also has the same type. 
* If $T_1,T_2$ have the same type, then the product $T=T_1 T_2$ also has the same type.
* $\inverse{T}$ has the same type as $T$.
* $\determinant{T} = \prod T_{ii}$, the product of its diagonal.  

#### Solving Triangular Matrices

Triangular matrices can be solved very quickly using backward/forward substitution. Consider solving
$U\mb{x} = \mb{y}$.  The bottom row only has one one non-zero element $U_{nn} x_n = y_n$, so
\eq{
    x_n = y_n / U_{nn}
}.  The next row has two:
\eqa {
    U_{n-1,n-1}x_{n-1} & + U_{n,n}x_n = y_{n-1} \\
    x_{n-1} & = \func{y_{n-1} - U_{n,n}x_n}/U_{n-1,n-1} 
}
and we know $x_n$ from the previous step.  We continue on recursively.  An upper triangular matrix
is solved the identical way except starting at the top row instead of the bottom.  Clearly the 
process breaks down if $T$ has a zero element on the diagonal, but we expected that since this
would cause $\determinant{T}=0$. A triangular matrix of size $N$ will take $order{N^2}$ operations
to solve in this way.  Consider a particular row in the algorithm described above.  The operations are
\eqa{
    y_i = & \sum_{j=1}^{N} U_{ij} x_j \\
        = & \sum_{j=i}^{N} U_{ij} x_j \\
        = & U_{ii} x_i + \sum_{j=i+1}^{N} U_{ij} x_j \\
    x_i = & \func{y_i - \sum_{j=i+1}^{N} U_{ij} x_j} / U_{ii}
}
where the second line follows since the first elements $U_{i1}$ to $U_{i,i-1}$ are all zero, and in 
last last recognise that we have already found the next elements we are solving for, so that for 
$x_i$ we already have found $x_{i+1}, x_{i+2}, \hdots, x_N$.   Computation of $x_i$ involves 
the summation of $U_{ij} x_j$ for $j=i+1,\hdots,N$, which is $\order{N}$.  This calculation is 
performed $N$ times, once for each of the $N$ rows, so in total we require 
$N \order{N} = \order{N^2}$ operations.

#### Creating Triangular Matrices with LU decompositions

The LU decomposition of a matrix $A$ are the upper and lower triangular matrices $U$ and $L$
where $A=LU$.  Conventionally, the main diagonal of $L$ is $L_{ii}=1$.  This ensures uniqueness,
since $A$ has $n^2$ elements, while $L$ has 
\eqa{
    \textrm{Elements} & = n + (n-1) + \hdots + 2 + 1 \\
                      & = \sum_{i=1}^{n} i \\
                      & = n(n+1)/2
}
non-zero elements, as does $U$.  So we have $n(n+1)$ non-zero variables in total to set using the 
$n^2$ elements of $A$.  To make this match set $n$ of them, the diagonal of $U$, to be all one.
When computing $L$ and $U$ analytically, we use sequences of transformations via elementary matrices.  In
practice $LU$ are computed numerically in $\order(N^3)$ operations.  Here, I should add both
numeric and analytic examples of LU decompositions.  Note that many authors refer to decompositions
where rows have been swapped (ie *pivoting*) as a *LU decomposition with pivoting*.  If a $LU$ 
decomposition is attempted directly without any pivoting the algorithm will fail anywhere with a 
zero element on the main diagonal.  For example the algorithm will fail to decompose
\eq{
    A = \mat{0 & 1 \\ 1 & 0}
}
where it is obvious that exchanging the rows leads to an easy solution.  If the pivot operations
on $A$ are collected in the matrix $P$, the the decomposition is $PA=LU$.  Here $P$ is *not*-unique,
but once $P$ is specified $LU$ is.  

The benefit of the $LU$ decomposition $PA=LU$ is that we can use forward and backward substitution
to very quickly solve linear systems.  Consider the system $A\mb{x}=\mb{y}$.  To solve, find $P$
which decomposes $PA=LU$, and solve $PA\mb{x}=LU\mb{x}=P\mb{y}$ in two steps.  The trick is to let 
$U\mb{x}=\mb{v}$, and solve in two steps as follows
\eqa{
    A\mb{x} = & \mb{y} \\
    PA\mb{x} = & P \mb{y} \\
    L\func{U\mb{x}} = &P\mb{y} \\
    L\mb{v} = &P\mb{y}
}
for $\mb{v}$, and then solve 
\eq{
    U\mb{x} = \mb{v}
}
for $\mb{x}$, as required.

### Eigenvalues and Eigenvectors

*Eigenvectors* and *Eigenvalues* of a matrix $A$ are fundamental quantities used regularly in
matrix operations. The Eigenvector/value pair corresponding to a matrix $A$ is the pair 
$\mb{v}$,$\lambda$ with the property that
\eq{
    A\mb{v} = \lambda \mb{v}
}
The geometric interpretation of this is nice, but I've never found it particularly usefull.  In
two dimensions $\mb{v}$ is a line in the $x-y$ plane, and $A\mb{x}$ is a linear transformation
that changes the `direction` $\mb{v}$ points, as well as its length.  If $\mb{v}$ is an is an
eigenvector of $A$, then the transformation $A\mb{v}$ only changes the length of the line $\mb{v}$,
and not its direction.  What is the practical importance of that?  I'm not really sure.  It 
is quite tedious to find eigenvalue/vector pairs analytically (include a few examples).  Computing 
eigenvectors/values numerically is not as simple as the $LU$ decomposition described earlier either.
Since they are so critically important in all areas of applied maths/statistics, the computation of
eigenvalues is a huge field of study.   Note that if $\lambda$ is an eigenvalue with a corresponding 
eigenvector $\mb{v}$, then any multiple of $\lambda$ is also an eigenvalue.  As such two vectors
$\mb{u}$ and $\mb{v}$ with the above property, where $\mb{u}=c\mb{v}$ (ie one is just a multiple
of the other) do not count as separate eigenvectors.  To see this note that 
\eqa{
    A\mb{v} = & \lambda \mb{v} \\
    A\func{c\mb{v}} = & \func{c\lambda}\mb{v} \\
    A\mb{u} = & \lambda_u \mb{u}
}
where $\mb{u} = c\mb{v}$ and $\lambda_u = c\lambda$.

To finding eigenvalue/vector pairs, note that if $\lambda$ is an eigenvalue of $A$, then
\eqa{
    A\mb{v} = & \lambda \mb{v} \\ 
    \func{A-\lambda I} \mb{v} = & 0
}
which means the vector $\mb{v}$ is in the null-space of the matrix $A-\lambda I$.  If such a vector
exists, so that the null-space is non-empty, we know $\func{A-\lambda I}$ is a singular matrix.
Since the purpose is to find such a $\mb{v}$, we need to find the conditions for which 
$\func{A-\lambda I}$ is singular.  The most obvious is that it has a zero determinant; ie
\eq{
    \determinant{A-\lambda I} = 0
}
This results in a polynomial equation for $\lambda$, which may have multiple solutions, some of
which might be repeated.  This equation is refered to as the *characteristic polynomial* of $A$.  Once 
we have all the solutions $\lambda_i$, we can work out the corresponding $\mb{v}_i$.  These are the eigenvalue/vector pairs.  This is quite a tedious process!  I should put in an example or two here.

Since computing the $\lambda_i$ reduces to working with determinants, the special structure of some
matrices means that computing the $\lambda_i$ is straight forward.  The subtraction $A-\lambda I$ only 
modifies the diagonal of $A$, many matrices retain the special structure after the subtraction.  Most 
importantly:

* **Diagonal Matrices** If $D$ is diagonal, then $A-\lambda I$ is diagonal too.  The determinant of
a diagonal matrix is $\determinant{D}=\prod D_{ii}$, so the characterist polynomial of $A-\lambda I$
is $\determinant{A-\lambda I} = \prod \func{D_{ii} - \lambda} = 0$. As such the diagonal elements
$D_{ii}$ are themselves the eigenvalues of $D$.

* **Triangular Matrices** If $T$ is triangular, then $A-\lambda I$ is triangular too.  The determinant of
a triangular matrix is $\determinant{T}=\prod T_{ii}$, so the characterist polynomial of $A-\lambda I$
is $\determinant{A-\lambda I} = \prod \func{T_{ii} - \lambda} = 0$. As such the diagonal elements
$T_{ii}$ are themselves the eigenvalues of $T$.

Eigenvector/value pairs give a great deal of information about the matrix they were computed from.  Let
$\func{ \lambda_{i}, \mb{v}_{i} }$ be the eigenvalue/vector pairs of the matrix $A$.  

* If any $\lambda_i=0$, then $A$ is singular.  Remember that if $A$ is non-singular, then it has 
an empty null-space.  That is, there can be no non-trivial vector $\mb{v}$ where $A\mb{v}=0$.  
However, if $\lambda=0$ is an eigenvalue, and $\mb{v}$ its corresponding eigenvector, then
$A\mb{v} = \lambda \mb{v} = 0$.  This means the null-space of $A$ is not empty (ie the 
eigenvector $\mb{v}$ is in it), so $A$ is singular.
* An $\func{n\times n}$ matrix A has exactly $n$ eigenvalues.  Some may be repeated, and some may
be complex.
* Every eigenvalue has at least one $eigenvector$.
* If $\lambda_1 \neq \lambda_2$ then their corresponding eigenvectors are linearly independant.
* If $A$ has an eigenvalue with multiplicity $m$, it has at least one eigenvector, but may have 
less than $m$ eigenvectors.  This means that $A$ may have $n$ eigenvectors which are all linearly
independant, but it may have less if any eigenvalues were repeated.
  